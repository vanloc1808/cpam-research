<!DOCTYPE html><!--zmzZAR0n9UDrTt1PnQ6CD--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/eraser.svg"/><link rel="preload" as="image" href="/results/img_1_original.png"/><link rel="preload" as="image" href="/results/img_2_original.png"/><link rel="preload" as="image" href="/results/img_3_original.png"/><link rel="preload" as="image" href="/results/img_4_original.png"/><link rel="preload" as="image" href="/results/img_5_original.png"/><link rel="preload" as="image" href="/results/img_6_original.png"/><link rel="preload" as="image" href="/results/img_7_original.png"/><link rel="preload" as="image" href="/results/img_8_original.png"/><link rel="preload" as="image" href="/results/img_9_original.png"/><link rel="stylesheet" href="/_next/static/chunks/9dd8c309db09461a.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/fd2be76e6b40ca47.js"/><script src="/_next/static/chunks/224331449d7c7e57.js" async=""></script><script src="/_next/static/chunks/2af9fca721db194b.js" async=""></script><script src="/_next/static/chunks/turbopack-480bbffd7a1ec151.js" async=""></script><script src="/_next/static/chunks/060f9a97930f3d04.js" async=""></script><script src="/_next/static/chunks/f146593b1533cb61.js" async=""></script><link rel="preload" as="image" href="/results/img_10_original.png"/><link rel="preload" as="image" href="/results/img_11_original.png"/><link rel="preload" as="image" href="/results/img_12_original.png"/><link rel="preload" as="image" href="/pipeline.png"/><link rel="preload" as="image" href="/qualitative.png"/><meta name="next-size-adjust" content=""/><title>PANDORA: Pixel-wise Attention Dissolution and Latent Guidance for Zero-Shot Object Removal</title><meta name="description" content="PANDORA: A zero-shot object removal framework that operates directly on pre-trained diffusion models using Pixel-wise Attention Dissolution and Localized Attentional Disentanglement Guidance."/><link rel="shortcut icon" href="/eraser.svg"/><link rel="icon" href="/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><link rel="icon" href="/eraser.svg"/><link rel="apple-touch-icon" href="/eraser.svg"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased"><div hidden=""><!--$--><!--/$--></div><div class="min-h-screen bg-slate-700 text-white"><section class="py-12 px-4"><div class="max-w-6xl mx-auto text-center"><h1 class="text-4xl md:text-5xl lg:text-5xl font-normal leading-tight mb-8 px-4 flex flex-col items-center"><span class="flex items-center gap-2">PANDORA<img src="/eraser.svg" alt="Eraser" class="w-10 h-10 md:w-12 md:h-12 lg:w-14 lg:h-14 inline-block"/>: Pixel-wise Attention Dissolution</span><span>and Latent Guidance for Zero-Shot Object Removal</span></h1><div class="text-lg md:text-xl mb-6 flex flex-wrap items-center justify-center gap-x-6 gap-y-2"><span><a href="https://orcid.org/0000-0001-8831-8846" target="_blank" rel="noopener noreferrer" class="text-sky-300 hover:text-sky-200 hover:underline">Dinh-Khoi Vo</a><sup>1,2</sup></span><span><a href="https://orcid.org/0000-0001-9351-3750" target="_blank" rel="noopener noreferrer" class="text-sky-300 hover:text-sky-200 hover:underline">Van-Loc Nguyen</a><sup>1,2</sup></span><span><a href="#" target="_blank" rel="noopener noreferrer" class="text-sky-300 hover:text-sky-200 hover:underline">Tam V. Nguyen</a><sup>3</sup></span><span><a href="https://orcid.org/0000-0003-3046-3041" target="_blank" rel="noopener noreferrer" class="text-sky-300 hover:text-sky-200 hover:underline">Minh-Triet Tran</a><sup>1,2</sup></span><span><a href="https://orcid.org/0000-0002-7363-2610" target="_blank" rel="noopener noreferrer" class="text-sky-300 hover:text-sky-200 hover:underline">Trung-Nghia Le</a><sup>1,2</sup></span></div><div class="text-base md:text-lg mb-4 flex flex-col items-center justify-center gap-y-2"><span><sup>1</sup>University of Science, VNU-HCM, Ho Chi Minh City, Vietnam</span><span><sup>2</sup>Vietnam National University, Ho Chi Minh City, Vietnam</span><span><sup>3</sup>University of Dayton, Ohio, United States</span></div><div class="text-base mb-8 italic">{vdkhoi, nvloc}@selab.hcmus.edu.vn, tamnguyen@udayton.edu, {tmtriet, ltnghia}@fit.hcmus.edu.vn</div><div class="flex flex-wrap items-center justify-center gap-8 mb-12"><a href="/PANDORA.pdf" class="px-6 py-3 bg-slate-600 hover:bg-slate-500 rounded border border-slate-500 transition-colors" rel="noopener noreferrer">PDF</a><a href="#" class="px-6 py-3 bg-slate-600 hover:bg-slate-500 rounded border border-slate-500 transition-colors cursor-not-allowed opacity-75 relative" title="Coming soon">arXiv<span class="absolute -top-1 -right-1 bg-red-500 text-white text-xs px-1 py-0.5 rounded-full font-bold transform rotate-12">Soon</span></a><a href="#" class="px-6 py-3 bg-slate-600 hover:bg-slate-500 rounded border border-slate-500 transition-colors cursor-not-allowed opacity-75 relative" title="Coming soon">Code<span class="absolute -top-1 -right-1 bg-red-500 text-white text-xs px-1 py-0.5 rounded-full font-bold transform rotate-12">Soon</span></a><a href="https://4e5fb5a771209acf2f.gradio.live/" class="px-6 py-3 bg-slate-600 hover:bg-slate-500 rounded border border-slate-500 transition-colors" target="_blank" rel="noopener noreferrer">Gradio Demo</a><a href="https://docs.google.com/forms/d/e/1FAIpQLSf6j6FOHiCdcZJJJ5DVRVgFTxIPTzEH91o2XVbsFNU6Xpp9Ig/viewform" class="px-6 py-3 bg-slate-600 hover:bg-slate-500 rounded border border-slate-500 transition-colors flex items-center gap-2" target="_blank" rel="noopener noreferrer"><span>üí¨</span>Feedback</a></div><div class="max-w-4xl mx-auto bg-gray-100 rounded-lg p-8"><h2 class="text-3xl text-gray-700 font-light mb-6">Demo Video</h2><div class="aspect-video bg-white rounded overflow-hidden relative"><video class="w-full h-full object-cover" controls="" loop="" muted="" playsInline=""><source src="/demo_video.mp4" type="video/mp4"/>Your browser does not support the video tag.</video><div class="absolute top-4 left-4 bg-black/70 text-white px-3 py-1 rounded text-sm font-medium">Original</div><div class="absolute top-4 right-4 bg-black/70 text-white px-3 py-1 rounded text-sm font-medium">Result</div></div></div></div></section><section class="py-12 px-4 bg-gray-100 text-gray-900"><div class="max-w-6xl mx-auto"><div class="flex gap-8"><h2 class="text-3xl font-bold uppercase min-w-fit">Abstract</h2><p class="text-lg leading-relaxed">Removing objects from natural images remains a formidable challenge, often hindered by the inability to synthesize semantically appropriate content in the foreground while preserving background integrity. Existing methods often rely on fine-tuning, prompt engineering, or inference-time optimization, yet still struggle to maintain texture consistency, produce rigid or unnatural results, lack precise foreground-background disentanglement, and fail to flexibly handle multiple objects‚Äîultimately limiting their scalability and practical applicability. In this paper, we propose a zero-shot object removal framework that operates directly on pre-trained text-to-image diffusion models‚Äîrequiring no fine-tuning, no prompts, and no optimization. At the core is our Pixel-wise Attention Dissolution, which performs fine-grained, pixel-wise dissolution of object information by nullifying the most correlated keys for each masked pixel. This operation causes the object to vanish from the self-attention flow, allowing the coherent background context to seamlessly dominate the reconstruction. To complement this, we introduce Localized Attentional Disentanglement Guidance, which steers the denoising process toward latent manifolds that favor clean object removal. Together, Pixel-wise Attention Dissolution and Localized Attentional Disentanglement Guidance enable precise, non-rigid, scalable, and prompt-free multi-object erasure in a single pass. Experiments show our method outperforms state-of-the-art methods even with fine-tuned and prompt-guided baselines in both visual fidelity and semantic plausibility.</p></div></div></section><section class="py-16 px-4 bg-white text-gray-900"><div class="max-w-6xl mx-auto"><h2 class="text-3xl font-bold uppercase text-center mb-6">Object Removal</h2><p class="text-lg text-center max-w-4xl mx-auto leading-relaxed">We propose a zero-shot object removal framework that operates directly on pre-trained diffusion models in a single pass, without any fine-tuning, prompt engineering, or inference-time optimization, thus fully leveraging their latent generative capacity for inpainting</p></div></section><section class="pt-2 pb-16 px-4 bg-gray-50 text-gray-900"><div class="max-w-6xl mx-auto"><div class="bg-green-50 border border-green-200 rounded-lg py-4 px-6 mb-8 text-center"><p class="text-green-900 flex items-center justify-center gap-2"><span class="text-2xl">üñ±Ô∏è</span><span>Click to see results</span></p><p class="text-green-700 text-sm mt-2 flex items-center justify-center gap-2"><span class="text-lg">‚è±Ô∏è</span><span>Processing takes ~10 seconds - please be patient!</span></p></div><div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-4 gap-6 "><div class="relative rounded-lg overflow-hidden cursor-pointer bg-gray-100 shadow-md hover:shadow-xl transition-shadow inline-block"><img src="/results/img_1_original.png" alt="" class="block select-none"/></div><div class="relative rounded-lg overflow-hidden cursor-pointer bg-gray-100 shadow-md hover:shadow-xl transition-shadow inline-block"><img src="/results/img_2_original.png" alt="" class="block select-none"/></div><div class="relative rounded-lg overflow-hidden cursor-pointer bg-gray-100 shadow-md hover:shadow-xl transition-shadow inline-block"><img src="/results/img_3_original.png" alt="" class="block select-none"/></div><div class="relative rounded-lg overflow-hidden cursor-pointer bg-gray-100 shadow-md hover:shadow-xl transition-shadow inline-block"><img src="/results/img_4_original.png" alt="" class="block select-none"/></div><div class="relative rounded-lg overflow-hidden cursor-pointer bg-gray-100 shadow-md hover:shadow-xl transition-shadow inline-block"><img src="/results/img_5_original.png" alt="" class="block select-none"/></div><div class="relative rounded-lg overflow-hidden cursor-pointer bg-gray-100 shadow-md hover:shadow-xl transition-shadow inline-block"><img src="/results/img_6_original.png" alt="" class="block select-none"/></div><div class="relative rounded-lg overflow-hidden cursor-pointer bg-gray-100 shadow-md hover:shadow-xl transition-shadow inline-block"><img src="/results/img_7_original.png" alt="" class="block select-none"/></div><div class="relative rounded-lg overflow-hidden cursor-pointer bg-gray-100 shadow-md hover:shadow-xl transition-shadow inline-block"><img src="/results/img_8_original.png" alt="" class="block select-none"/></div><div class="relative rounded-lg overflow-hidden cursor-pointer bg-gray-100 shadow-md hover:shadow-xl transition-shadow inline-block"><img src="/results/img_9_original.png" alt="" class="block select-none"/></div><div class="relative rounded-lg overflow-hidden cursor-pointer bg-gray-100 shadow-md hover:shadow-xl transition-shadow inline-block"><img src="/results/img_10_original.png" alt="" class="block select-none"/></div><div class="relative rounded-lg overflow-hidden cursor-pointer bg-gray-100 shadow-md hover:shadow-xl transition-shadow inline-block"><img src="/results/img_11_original.png" alt="" class="block select-none"/></div><div class="relative rounded-lg overflow-hidden cursor-pointer bg-gray-100 shadow-md hover:shadow-xl transition-shadow inline-block"><img src="/results/img_12_original.png" alt="" class="block select-none"/></div></div></div></section><section class="py-16 px-4 bg-gray-100 text-gray-900"><div class="max-w-6xl mx-auto"><h2 class="text-3xl font-bold uppercase text-center mb-6">Approach</h2><div><p class="text-lg leading-relaxed mb-6">Our framework performs zero-shot object removal directly on a pre-trained diffusion model. Given an input image <i>I<sub>s</sub></i> and a binary mask <i>M</i> specifying the target objects, the model produces an edited image <i>I<sub>t</sub></i> where the masked regions are erased and seamlessly reconstructed with contextually consistent background. The process begins with latent inversion to map the input image into the noise space while preserving unaffected regions in the denoising process. We then apply <strong>Pixel-wise Attention Dissolution (PAD)</strong> to disconnect masked query pixels from their most correlated keys, effectively dissolving object information at the attention level. Next, <strong>Localized Attentional Disentanglement Guidance (LADG)</strong> steers the denoising trajectory in latent space away from the object regions, refining the reconstruction to suppress residual artifacts.</p><p class="text-lg leading-relaxed mb-6">Together, PAD and LADG enable precise, pixel-level control for single- and multi-object removal in a single forward pass, without any fine-tuning, prompt engineering, or inference-time optimization.</p><div class="bg-white rounded-lg p-6 border border-gray-200"><img src="/pipeline.png" alt="PANDORA Pipeline Diagram" class="w-full h-auto rounded"/></div></div></div></section><section class="py-16 px-4 bg-white text-gray-900"><div class="max-w-7xl mx-auto"><h2 class="text-3xl font-bold uppercase text-center mb-6">Qualitative Comparison</h2><div class="max-w-5xl mx-auto mb-8"><div class="bg-blue-50 border border-blue-200 rounded-lg p-6 mb-6"><p class="text-lg text-center mb-4 text-blue-900"><span class="flex items-center justify-center gap-2 mb-2"><span class="text-2xl">üî¨</span><span class="font-semibold">Qualitative comparison on various object removal scenarios</span></span><span class="flex items-center justify-center gap-2 text-base"><span class="text-xl">üìä</span><span>From left to right: original image with a mask, and results from different methods</span></span></p></div><div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-6"><div class="bg-green-50 border border-green-200 rounded-lg p-4 text-center"><div class="text-2xl mb-2">üéØ</div><h3 class="font-semibold text-green-900 mb-1">Single-Object Removal</h3><p class="text-sm text-green-700">Top two rows</p></div><div class="bg-orange-50 border border-orange-200 rounded-lg p-4 text-center"><div class="text-2xl mb-2">üéØüéØ</div><h3 class="font-semibold text-orange-900 mb-1">Multi-Object Cases</h3><p class="text-sm text-orange-700">Middle two rows</p></div><div class="bg-purple-50 border border-purple-200 rounded-lg p-4 text-center"><div class="text-2xl mb-2">üéØüéØüéØ</div><h3 class="font-semibold text-purple-900 mb-1">Mass-Similar Objects</h3><p class="text-sm text-purple-700">Bottom two rows</p></div></div><div class="bg-gray-50 border border-gray-200 rounded-lg p-4 text-center"><p class="text-gray-800"><span class="text-xl mr-2">‚ö°</span><span class="font-semibold">Zero-shot methods</span> shown in the last four columns, with the <span class="font-bold text-blue-600">last two columns showing our PANDORA method</span></p></div></div><div class="bg-gray-50 rounded-lg p-6 border border-gray-200"><img src="/qualitative.png" alt="Qualitative comparison of object removal methods" class="w-full h-auto rounded shadow-lg"/></div></div></section><section class="py-16 px-4 bg-gray-50 text-gray-900"><div class="max-w-7xl mx-auto"><h2 class="text-3xl font-bold uppercase text-center mb-6">Quantitative Comparison</h2><div class="bg-white rounded-lg p-6 border border-gray-200"><div class="w-full overflow-x-auto"><table class="min-w-full divide-y divide-gray-200 text-sm md:text-base"><thead class="bg-gray-100"><tr><th scope="col" class="py-3 px-4 text-left font-semibold">Method</th><th scope="col" class="py-3 px-4 text-center font-semibold">Text</th><th scope="col" class="py-3 px-4 text-center font-semibold">FID‚Üì</th><th scope="col" class="py-3 px-4 text-center font-semibold">LPIPS‚Üì</th><th scope="col" class="py-3 px-4 text-center font-semibold">MSE‚Üì</th><th scope="col" class="py-3 px-4 text-center font-semibold">CLIP score‚Üë</th></tr></thead><tbody><tr><td colSpan="6" class="bg-gray-50 text-gray-700 italic text-center py-2">Fine-tuning-based methods (SD 2.1 backbone, except LaMa)</td></tr><tr><td class="py-3 px-4 font-medium whitespace-nowrap">PowerPaint</td><td class="py-3 px-4 text-center">‚úî</td><td class="py-3 px-4 text-center"><span>22.81</span></td><td class="py-3 px-4 text-center"><span>0.1322</span></td><td class="py-3 px-4 text-center"><span>0.0104</span></td><td class="py-3 px-4 text-center"><span>24.15</span></td></tr><tr><td class="py-3 px-4 font-medium whitespace-nowrap">LaMa</td><td class="py-3 px-4 text-center">‚úò</td><td class="py-3 px-4 text-center"><span class="font-bold">0.71</span></td><td class="py-3 px-4 text-center"><span class="font-bold">0.0012</span></td><td class="py-3 px-4 text-center"><span class="font-bold">0.0001</span></td><td class="py-3 px-4 text-center"><span>24.5</span></td></tr><tr><td class="py-3 px-4 font-medium whitespace-nowrap">SD2-Inpaint</td><td class="py-3 px-4 text-center">‚úò</td><td class="py-3 px-4 text-center"><span>17.93</span></td><td class="py-3 px-4 text-center"><span>0.1106</span></td><td class="py-3 px-4 text-center"><span>0.0073</span></td><td class="py-3 px-4 text-center"><span>24.06</span></td></tr><tr><td class="py-3 px-4 font-medium whitespace-nowrap">SD2-Inpaint-wprompt</td><td class="py-3 px-4 text-center">‚úî</td><td class="py-3 px-4 text-center"><span>18.01</span></td><td class="py-3 px-4 text-center"><span>0.1098</span></td><td class="py-3 px-4 text-center"><span>0.0072</span></td><td class="py-3 px-4 text-center"><span>24.32</span></td></tr></tbody><tbody><tr><td colSpan="6" class="bg-gray-50 text-gray-700 italic text-center py-2">Zero-shot methods (no retraining, SD 2.1 backbone)</td></tr><tr><td class="py-3 px-4 font-medium whitespace-nowrap">CPAM</td><td class="py-3 px-4 text-center">‚úò</td><td class="py-3 px-4 text-center"><span>25.25</span></td><td class="py-3 px-4 text-center"><span>0.0953</span></td><td class="py-3 px-4 text-center"><span>0.0048</span></td><td class="py-3 px-4 text-center"><span>24.49</span></td></tr><tr class="bg-yellow-50"><td class="py-3 px-4 font-medium whitespace-nowrap">PANDORA w/o PAD (Ours)</td><td class="py-3 px-4 text-center">‚úò</td><td class="py-3 px-4 text-center"><span>27.3</span></td><td class="py-3 px-4 text-center"><span>0.0985</span></td><td class="py-3 px-4 text-center"><span>0.005</span></td><td class="py-3 px-4 text-center"><span>24.58</span></td></tr><tr class="bg-yellow-50"><td class="py-3 px-4 font-medium whitespace-nowrap">PANDORA w/o LADG (Ours)</td><td class="py-3 px-4 text-center">‚úò</td><td class="py-3 px-4 text-center"><span>30.8</span></td><td class="py-3 px-4 text-center"><span>0.1007</span></td><td class="py-3 px-4 text-center"><span>0.0055</span></td><td class="py-3 px-4 text-center"><span>24.65</span></td></tr><tr class="bg-yellow-50"><td class="py-3 px-4 font-medium whitespace-nowrap">PANDORA (Ours)</td><td class="py-3 px-4 text-center">‚úò</td><td class="py-3 px-4 text-center"><span>35.1</span></td><td class="py-3 px-4 text-center"><span>0.1064</span></td><td class="py-3 px-4 text-center"><span>0.0059</span></td><td class="py-3 px-4 text-center"><span class="font-bold">24.69</span></td></tr></tbody><tbody><tr><td colSpan="6" class="bg-gray-50 text-gray-700 italic text-center py-2">Zero-shot methods (no retraining, SD 1.5 backbone)</td></tr><tr><td class="py-3 px-4 font-medium whitespace-nowrap">CPAM</td><td class="py-3 px-4 text-center">‚úò</td><td class="py-3 px-4 text-center"><span>29.54</span></td><td class="py-3 px-4 text-center"><span>0.1564</span></td><td class="py-3 px-4 text-center"><span>0.0138</span></td><td class="py-3 px-4 text-center"><span>24.32</span></td></tr><tr><td class="py-3 px-4 font-medium whitespace-nowrap">Attentive Eraser</td><td class="py-3 px-4 text-center">‚úò</td><td class="py-3 px-4 text-center"><span>118.09</span></td><td class="py-3 px-4 text-center"><span>0.2567</span></td><td class="py-3 px-4 text-center"><span>0.027</span></td><td class="py-3 px-4 text-center"><span>24.42</span></td></tr><tr class="bg-yellow-50"><td class="py-3 px-4 font-medium whitespace-nowrap">PANDORA w/o PAD (Ours)</td><td class="py-3 px-4 text-center">‚úò</td><td class="py-3 px-4 text-center"><span>35.59</span></td><td class="py-3 px-4 text-center"><span>0.1702</span></td><td class="py-3 px-4 text-center"><span>0.0156</span></td><td class="py-3 px-4 text-center"><span>24.4</span></td></tr><tr class="bg-yellow-50"><td class="py-3 px-4 font-medium whitespace-nowrap">PANDORA w/o LADG (Ours)</td><td class="py-3 px-4 text-center">‚úò</td><td class="py-3 px-4 text-center"><span>42.17</span></td><td class="py-3 px-4 text-center"><span>0.1844</span></td><td class="py-3 px-4 text-center"><span>0.0171</span></td><td class="py-3 px-4 text-center"><span>24.55</span></td></tr><tr class="bg-yellow-50"><td class="py-3 px-4 font-medium whitespace-nowrap">PANDORA (Ours)</td><td class="py-3 px-4 text-center">‚úò</td><td class="py-3 px-4 text-center"><span>44.98</span></td><td class="py-3 px-4 text-center"><span>0.1895</span></td><td class="py-3 px-4 text-center"><span>0.0184</span></td><td class="py-3 px-4 text-center"><span>24.57</span></td></tr></tbody></table><div class="max-w-4xl mx-auto mt-4"><div class="bg-blue-50 border border-blue-200 rounded-lg p-4 text-sm text-blue-900"><div class="flex flex-col gap-2 sm:flex-row sm:items-center sm:justify-center sm:gap-6 text-center"><div class="flex items-center justify-center gap-2"><span>üìâ</span><span>Lower is better: <span class="font-medium">FID</span>, <span class="font-medium">LPIPS</span>, <span class="font-medium">MSE</span></span></div><div class="hidden sm:block text-blue-300">|</div><div class="flex items-center justify-center gap-2"><span>üìà</span><span>Higher is better: <span class="font-medium">CLIP score</span></span></div></div><div class="mt-3 flex flex-col gap-2 sm:flex-row sm:items-center sm:justify-center sm:gap-6 text-center"><div class="flex items-center justify-center gap-2"><span>üìù</span><span><span class="font-medium">Text</span> column: <span class="font-semibold">‚úî</span> uses prompts; <span class="font-semibold">‚úò</span> no prompts</span></div><div class="hidden sm:block text-blue-300">|</div><div class="flex items-center justify-center gap-2"><span>‚≠ê</span><span>Bold numbers indicate best across all methods; yellow rows are <span class="font-semibold">Ours</span></span></div></div><p class="mt-3 text-center text-blue-800">Quantitative comparison of fine-tuned and zero-shot object removal methods averaged across all dataset types. PANDORA consistently achieves the best object removal quality with competitive background realism, without any retraining or textual prompts, demonstrating strong generalization across both Stable Diffusion v1.5 and v2.1 backbones. Removing LADG slightly reduces removal quality, while removing PAD causes a significant degradation.</p></div></div></div></div></div></section><section class="py-16 px-4 bg-gray-200 text-gray-900"><div class="max-w-6xl mx-auto"><h2 class="text-3xl font-bold uppercase text-center mb-6">Acknowledgment</h2><div class="flex-1"><div class="bg-amber-50 border border-amber-200 rounded-lg p-6 mb-6"><div class="flex items-center gap-3 mb-4"><span class="text-3xl">üí∞</span><h3 class="text-xl font-semibold text-amber-900">Funding and GPU Support</h3></div><p class="text-lg leading-relaxed text-amber-800">This research is funded by the Vietnam National Foundation for Science and Technology Development (NAFOSTED) under Grant Number 102.05-2023.31. This research used the GPUs provided by the Intelligent Systems Lab at the Faculty of Information Technology, University of Science, VNU-HCM.</p></div><div class="bg-blue-50 border border-blue-200 rounded-lg p-6 mb-6"><div class="flex items-center gap-3 mb-4"><span class="text-3xl">üôè</span><h3 class="text-xl font-semibold text-blue-900">User Study Participants</h3></div><p class="text-lg leading-relaxed text-blue-800">We extend our heartfelt gratitude to all participants who took part in our comprehensive user study. Your valuable time, thoughtful feedback, and detailed evaluations were instrumental in validating the effectiveness and usability of our PANDORA framework. Your insights helped us understand the practical impact of our zero-shot object removal approach and provided crucial evidence of its superiority over existing methods.</p></div><div class="bg-green-50 border border-green-200 rounded-lg p-6 mb-6"><div class="flex items-center gap-3 mb-4"><span class="text-3xl">üé®</span><h3 class="text-xl font-semibold text-green-900">Website Design Inspiration</h3></div><p class="text-lg leading-relaxed text-green-800">This website design is inspired by<!-- --> <a href="https://objectdrop.github.io/" class="text-blue-600 hover:underline font-semibold" target="_blank" rel="noopener noreferrer">ObjectDrop</a>. We thank the authors for their excellent work and creative design approach.</p></div><div class="bg-purple-50 border border-purple-200 rounded-lg p-6"><div class="flex items-center gap-3 mb-4"><span class="text-3xl">üöÄ</span><h3 class="text-xl font-semibold text-purple-900">Demo Design Inspiration</h3></div><p class="text-lg leading-relaxed text-purple-800">Our Gradio demo design is inspired by<!-- --> <a href="https://huggingface.co/spaces/xichenhku/MimicBrush/tree/main" class="text-blue-600 hover:underline font-semibold" target="_blank" rel="noopener noreferrer">MimicBrush</a>. We thank the authors for their excellent work and creative design approach.</p></div></div></div></section></div><!--$--><!--/$--><script src="/_next/static/chunks/fd2be76e6b40ca47.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[39756,[\"/_next/static/chunks/060f9a97930f3d04.js\"],\"default\"]\n3:I[37457,[\"/_next/static/chunks/060f9a97930f3d04.js\"],\"default\"]\n5:I[97367,[\"/_next/static/chunks/060f9a97930f3d04.js\"],\"OutletBoundary\"]\n7:I[11533,[\"/_next/static/chunks/060f9a97930f3d04.js\"],\"AsyncMetadataOutlet\"]\n9:I[97367,[\"/_next/static/chunks/060f9a97930f3d04.js\"],\"ViewportBoundary\"]\nb:I[97367,[\"/_next/static/chunks/060f9a97930f3d04.js\"],\"MetadataBoundary\"]\nc:\"$Sreact.suspense\"\ne:I[68027,[],\"default\"]\n:HL[\"/_next/static/chunks/9dd8c309db09461a.css\",\"style\"]\n:HL[\"/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"zmzZAR0n9UDrTt1PnQ6CD\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/9dd8c309db09461a.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/f146593b1533cb61.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$L6\",[\"$\",\"$L7\",null,{\"promise\":\"$@8\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L9\",null,{\"children\":\"$La\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$c\",null,{\"fallback\":null,\"children\":\"$Ld\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/9dd8c309db09461a.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,"f:I[27201,[\"/_next/static/chunks/060f9a97930f3d04.js\"],\"IconMark\"]\n"])</script><script>self.__next_f.push([1,"8:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"PANDORA: Pixel-wise Attention Dissolution and Latent Guidance for Zero-Shot Object Removal\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"PANDORA: A zero-shot object removal framework that operates directly on pre-trained diffusion models using Pixel-wise Attention Dissolution and Localized Attentional Disentanglement Guidance.\"}],[\"$\",\"link\",\"2\",{\"rel\":\"shortcut icon\",\"href\":\"/eraser.svg\"}],[\"$\",\"link\",\"3\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/eraser.svg\"}],[\"$\",\"link\",\"5\",{\"rel\":\"apple-touch-icon\",\"href\":\"/eraser.svg\"}],[\"$\",\"$Lf\",\"6\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"d:\"$8:metadata\"\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-slate-700 text-white\",\"children\":[[\"$\",\"section\",null,{\"className\":\"py-12 px-4\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl md:text-5xl lg:text-5xl font-normal leading-tight mb-8 px-4 flex flex-col items-center\",\"children\":[[\"$\",\"span\",null,{\"className\":\"flex items-center gap-2\",\"children\":[\"PANDORA\",[\"$\",\"img\",null,{\"src\":\"/eraser.svg\",\"alt\":\"Eraser\",\"className\":\"w-10 h-10 md:w-12 md:h-12 lg:w-14 lg:h-14 inline-block\"}],\": Pixel-wise Attention Dissolution\"]}],[\"$\",\"span\",null,{\"children\":\"and Latent Guidance for Zero-Shot Object Removal\"}]]}],[\"$\",\"div\",null,{\"className\":\"text-lg md:text-xl mb-6 flex flex-wrap items-center justify-center gap-x-6 gap-y-2\",\"children\":[[\"$\",\"span\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://orcid.org/0000-0001-8831-8846\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-sky-300 hover:text-sky-200 hover:underline\",\"children\":\"Dinh-Khoi Vo\"}],[\"$\",\"sup\",null,{\"children\":\"1,2\"}]]}],[\"$\",\"span\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://orcid.org/0000-0001-9351-3750\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-sky-300 hover:text-sky-200 hover:underline\",\"children\":\"Van-Loc Nguyen\"}],[\"$\",\"sup\",null,{\"children\":\"1,2\"}]]}],[\"$\",\"span\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"#\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-sky-300 hover:text-sky-200 hover:underline\",\"children\":\"Tam V. Nguyen\"}],[\"$\",\"sup\",null,{\"children\":\"3\"}]]}],[\"$\",\"span\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://orcid.org/0000-0003-3046-3041\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-sky-300 hover:text-sky-200 hover:underline\",\"children\":\"Minh-Triet Tran\"}],[\"$\",\"sup\",null,{\"children\":\"1,2\"}]]}],[\"$\",\"span\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://orcid.org/0000-0002-7363-2610\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-sky-300 hover:text-sky-200 hover:underline\",\"children\":\"Trung-Nghia Le\"}],[\"$\",\"sup\",null,{\"children\":\"1,2\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"text-base md:text-lg mb-4 flex flex-col items-center justify-center gap-y-2\",\"children\":[[\"$\",\"span\",null,{\"children\":[[\"$\",\"sup\",null,{\"children\":\"1\"}],\"University of Science, VNU-HCM, Ho Chi Minh City, Vietnam\"]}],[\"$\",\"span\",null,{\"children\":[[\"$\",\"sup\",null,{\"children\":\"2\"}],\"Vietnam National University, Ho Chi Minh City, Vietnam\"]}],[\"$\",\"span\",null,{\"children\":[[\"$\",\"sup\",null,{\"children\":\"3\"}],\"University of Dayton, Ohio, United States\"]}]]}],[\"$\",\"div\",null,{\"className\":\"text-base mb-8 italic\",\"children\":\"{vdkhoi, nvloc}@selab.hcmus.edu.vn, tamnguyen@udayton.edu, {tmtriet, ltnghia}@fit.hcmus.edu.vn\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap items-center justify-center gap-8 mb-12\",\"children\":[[\"$\",\"a\",null,{\"href\":\"/PANDORA.pdf\",\"className\":\"px-6 py-3 bg-slate-600 hover:bg-slate-500 rounded border border-slate-500 transition-colors\",\"rel\":\"noopener noreferrer\",\"children\":\"PDF\"}],[\"$\",\"a\",null,{\"href\":\"#\",\"className\":\"px-6 py-3 bg-slate-600 hover:bg-slate-500 rounded border border-slate-500 transition-colors cursor-not-allowed opacity-75 relative\",\"title\":\"Coming soon\",\"children\":[\"arXiv\",[\"$\",\"span\",null,{\"className\":\"absolute -top-1 -right-1 bg-red-500 text-white text-xs px-1 py-0.5 rounded-full font-bold transform rotate-12\",\"children\":\"Soon\"}]]}],[\"$\",\"a\",null,{\"href\":\"#\",\"className\":\"px-6 py-3 bg-slate-600 hover:bg-slate-500 rounded border border-slate-500 transition-colors cursor-not-allowed opacity-75 relative\",\"title\":\"Coming soon\",\"children\":[\"Code\",[\"$\",\"span\",null,{\"className\":\"absolute -top-1 -right-1 bg-red-500 text-white text-xs px-1 py-0.5 rounded-full font-bold transform rotate-12\",\"children\":\"Soon\"}]]}],[\"$\",\"a\",null,{\"href\":\"https://4e5fb5a771209acf2f.gradio.live/\",\"className\":\"px-6 py-3 bg-slate-600 hover:bg-slate-500 rounded border border-slate-500 transition-colors\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Gradio Demo\"}],[\"$\",\"a\",null,{\"href\":\"https://docs.google.com/forms/d/e/1FAIpQLSf6j6FOHiCdcZJJJ5DVRVgFTxIPTzEH91o2XVbsFNU6Xpp9Ig/viewform\",\"className\":\"px-6 py-3 bg-slate-600 hover:bg-slate-500 rounded border border-slate-500 transition-colors flex items-center gap-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"$L10\",\"Feedback\"]}]]}],\"$L11\"]}]}],\"$L12\",\"$L13\",\"$L14\",\"$L15\",\"$L16\",\"$L17\",\"$L18\"]}]\n"])</script><script>self.__next_f.push([1,"1a:I[54818,[\"/_next/static/chunks/f146593b1533cb61.js\"],\"default\"]\n1b:I[40714,[\"/_next/static/chunks/f146593b1533cb61.js\"],\"default\"]\n10:[\"$\",\"span\",null,{\"children\":\"üí¨\"}]\n"])</script><script>self.__next_f.push([1,"11:[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto bg-gray-100 rounded-lg p-8\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl text-gray-700 font-light mb-6\",\"children\":\"Demo Video\"}],[\"$\",\"div\",null,{\"className\":\"aspect-video bg-white rounded overflow-hidden relative\",\"children\":[[\"$\",\"video\",null,{\"className\":\"w-full h-full object-cover\",\"controls\":true,\"loop\":true,\"muted\":true,\"playsInline\":true,\"children\":[[\"$\",\"source\",null,{\"src\":\"/demo_video.mp4\",\"type\":\"video/mp4\"}],\"Your browser does not support the video tag.\"]}],[\"$\",\"div\",null,{\"className\":\"absolute top-4 left-4 bg-black/70 text-white px-3 py-1 rounded text-sm font-medium\",\"children\":\"Original\"}],[\"$\",\"div\",null,{\"className\":\"absolute top-4 right-4 bg-black/70 text-white px-3 py-1 rounded text-sm font-medium\",\"children\":\"Result\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"19:T651,"])</script><script>self.__next_f.push([1,"Removing objects from natural images remains a formidable challenge, often hindered by the inability to synthesize semantically appropriate content in the foreground while preserving background integrity. Existing methods often rely on fine-tuning, prompt engineering, or inference-time optimization, yet still struggle to maintain texture consistency, produce rigid or unnatural results, lack precise foreground-background disentanglement, and fail to flexibly handle multiple objects‚Äîultimately limiting their scalability and practical applicability. In this paper, we propose a zero-shot object removal framework that operates directly on pre-trained text-to-image diffusion models‚Äîrequiring no fine-tuning, no prompts, and no optimization. At the core is our Pixel-wise Attention Dissolution, which performs fine-grained, pixel-wise dissolution of object information by nullifying the most correlated keys for each masked pixel. This operation causes the object to vanish from the self-attention flow, allowing the coherent background context to seamlessly dominate the reconstruction. To complement this, we introduce Localized Attentional Disentanglement Guidance, which steers the denoising process toward latent manifolds that favor clean object removal. Together, Pixel-wise Attention Dissolution and Localized Attentional Disentanglement Guidance enable precise, non-rigid, scalable, and prompt-free multi-object erasure in a single pass. Experiments show our method outperforms state-of-the-art methods even with fine-tuned and prompt-guided baselines in both visual fidelity and semantic plausibility."])</script><script>self.__next_f.push([1,"12:[\"$\",\"section\",null,{\"className\":\"py-12 px-4 bg-gray-100 text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex gap-8\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold uppercase min-w-fit\",\"children\":\"Abstract\"}],[\"$\",\"p\",null,{\"className\":\"text-lg leading-relaxed\",\"children\":\"$19\"}]]}]}]}]\n13:[\"$\",\"section\",null,{\"className\":\"py-16 px-4 bg-white text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold uppercase text-center mb-6\",\"children\":\"Object Removal\"}],[\"$\",\"p\",null,{\"className\":\"text-lg text-center max-w-4xl mx-auto leading-relaxed\",\"children\":\"We propose a zero-shot object removal framework that operates directly on pre-trained diffusion models in a single pass, without any fine-tuning, prompt engineering, or inference-time optimization, thus fully leveraging their latent generative capacity for inpainting\"}]]}]}]\n"])</script><script>self.__next_f.push([1,"14:[\"$\",\"section\",null,{\"className\":\"pt-2 pb-16 px-4 bg-gray-50 text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-green-50 border border-green-200 rounded-lg py-4 px-6 mb-8 text-center\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-green-900 flex items-center justify-center gap-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-2xl\",\"children\":\"üñ±Ô∏è\"}],[\"$\",\"span\",null,{\"children\":\"Click to see results\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-green-700 text-sm mt-2 flex items-center justify-center gap-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-lg\",\"children\":\"‚è±Ô∏è\"}],[\"$\",\"span\",null,{\"children\":\"Processing takes ~10 seconds - please be patient!\"}]]}]]}],[\"$\",\"$L1a\",null,{\"items\":[{\"id\":\"1\",\"originalSrc\":\"/results/img_1_original.png\",\"resultSrc\":\"/results/img_1_result.png\",\"maskSrc\":\"/results/img_1_mask.png\"},{\"id\":\"2\",\"originalSrc\":\"/results/img_2_original.png\",\"resultSrc\":\"/results/img_2_result.png\",\"maskSrc\":\"/results/img_2_mask.png\"},{\"id\":\"3\",\"originalSrc\":\"/results/img_3_original.png\",\"resultSrc\":\"/results/img_3_result.png\",\"maskSrc\":\"/results/img_3_mask.png\"},{\"id\":\"4\",\"originalSrc\":\"/results/img_4_original.png\",\"resultSrc\":\"/results/img_4_result.png\",\"maskSrc\":\"/results/img_4_mask.png\"},{\"id\":\"5\",\"originalSrc\":\"/results/img_5_original.png\",\"resultSrc\":\"/results/img_5_result.png\",\"maskSrc\":\"/results/img_5_mask.png\"},{\"id\":\"6\",\"originalSrc\":\"/results/img_6_original.png\",\"resultSrc\":\"/results/img_6_result.png\",\"maskSrc\":\"/results/img_6_mask.png\"},{\"id\":\"7\",\"originalSrc\":\"/results/img_7_original.png\",\"resultSrc\":\"/results/img_7_result.png\",\"maskSrc\":\"/results/img_7_mask.png\"},{\"id\":\"8\",\"originalSrc\":\"/results/img_8_original.png\",\"resultSrc\":\"/results/img_8_result.png\",\"maskSrc\":\"/results/img_8_mask.png\"},{\"id\":\"9\",\"originalSrc\":\"/results/img_9_original.png\",\"resultSrc\":\"/results/img_9_result.png\",\"maskSrc\":\"/results/img_9_mask.png\"},{\"id\":\"10\",\"originalSrc\":\"/results/img_10_original.png\",\"resultSrc\":\"/results/img_10_result.png\",\"maskSrc\":\"/results/img_10_mask.png\"},{\"id\":\"11\",\"originalSrc\":\"/results/img_11_original.png\",\"resultSrc\":\"/results/img_11_result.png\",\"maskSrc\":\"/results/img_11_mask.png\"},{\"id\":\"12\",\"originalSrc\":\"/results/img_12_original.png\",\"resultSrc\":\"/results/img_12_result.png\",\"maskSrc\":\"/results/img_12_mask.png\"}]}]]}]}]\n"])</script><script>self.__next_f.push([1,"15:[\"$\",\"section\",null,{\"className\":\"py-16 px-4 bg-gray-100 text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold uppercase text-center mb-6\",\"children\":\"Approach\"}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"p\",null,{\"className\":\"text-lg leading-relaxed mb-6\",\"children\":[\"Our framework performs zero-shot object removal directly on a pre-trained diffusion model. Given an input image \",[\"$\",\"i\",null,{\"children\":[\"I\",[\"$\",\"sub\",null,{\"children\":\"s\"}]]}],\" and a binary mask \",[\"$\",\"i\",null,{\"children\":\"M\"}],\" specifying the target objects, the model produces an edited image \",[\"$\",\"i\",null,{\"children\":[\"I\",[\"$\",\"sub\",null,{\"children\":\"t\"}]]}],\" where the masked regions are erased and seamlessly reconstructed with contextually consistent background. The process begins with latent inversion to map the input image into the noise space while preserving unaffected regions in the denoising process. We then apply \",[\"$\",\"strong\",null,{\"children\":\"Pixel-wise Attention Dissolution (PAD)\"}],\" to disconnect masked query pixels from their most correlated keys, effectively dissolving object information at the attention level. Next, \",[\"$\",\"strong\",null,{\"children\":\"Localized Attentional Disentanglement Guidance (LADG)\"}],\" steers the denoising trajectory in latent space away from the object regions, refining the reconstruction to suppress residual artifacts.\"]}],[\"$\",\"p\",null,{\"className\":\"text-lg leading-relaxed mb-6\",\"children\":\"Together, PAD and LADG enable precise, pixel-level control for single- and multi-object removal in a single forward pass, without any fine-tuning, prompt engineering, or inference-time optimization.\"}],[\"$\",\"div\",null,{\"className\":\"bg-white rounded-lg p-6 border border-gray-200\",\"children\":[\"$\",\"img\",null,{\"src\":\"/pipeline.png\",\"alt\":\"PANDORA Pipeline Diagram\",\"className\":\"w-full h-auto rounded\"}]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"16:[\"$\",\"section\",null,{\"className\":\"py-16 px-4 bg-white text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-7xl mx-auto\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold uppercase text-center mb-6\",\"children\":\"Qualitative Comparison\"}],[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto mb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-blue-50 border border-blue-200 rounded-lg p-6 mb-6\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-lg text-center mb-4 text-blue-900\",\"children\":[[\"$\",\"span\",null,{\"className\":\"flex items-center justify-center gap-2 mb-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-2xl\",\"children\":\"üî¨\"}],[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Qualitative comparison on various object removal scenarios\"}]]}],[\"$\",\"span\",null,{\"className\":\"flex items-center justify-center gap-2 text-base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xl\",\"children\":\"üìä\"}],[\"$\",\"span\",null,{\"children\":\"From left to right: original image with a mask, and results from different methods\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 md:grid-cols-3 gap-4 mb-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-green-50 border border-green-200 rounded-lg p-4 text-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-2xl mb-2\",\"children\":\"üéØ\"}],[\"$\",\"h3\",null,{\"className\":\"font-semibold text-green-900 mb-1\",\"children\":\"Single-Object Removal\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-green-700\",\"children\":\"Top two rows\"}]]}],[\"$\",\"div\",null,{\"className\":\"bg-orange-50 border border-orange-200 rounded-lg p-4 text-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-2xl mb-2\",\"children\":\"üéØüéØ\"}],[\"$\",\"h3\",null,{\"className\":\"font-semibold text-orange-900 mb-1\",\"children\":\"Multi-Object Cases\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-orange-700\",\"children\":\"Middle two rows\"}]]}],[\"$\",\"div\",null,{\"className\":\"bg-purple-50 border border-purple-200 rounded-lg p-4 text-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-2xl mb-2\",\"children\":\"üéØüéØüéØ\"}],[\"$\",\"h3\",null,{\"className\":\"font-semibold text-purple-900 mb-1\",\"children\":\"Mass-Similar Objects\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-purple-700\",\"children\":\"Bottom two rows\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-50 border border-gray-200 rounded-lg p-4 text-center\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-800\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xl mr-2\",\"children\":\"‚ö°\"}],[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Zero-shot methods\"}],\" shown in the last four columns, with the \",[\"$\",\"span\",null,{\"className\":\"font-bold text-blue-600\",\"children\":\"last two columns showing our PANDORA method\"}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-gray-50 rounded-lg p-6 border border-gray-200\",\"children\":[\"$\",\"img\",null,{\"src\":\"/qualitative.png\",\"alt\":\"Qualitative comparison of object removal methods\",\"className\":\"w-full h-auto rounded shadow-lg\"}]}]]}]}]\n"])</script><script>self.__next_f.push([1,"17:[\"$\",\"section\",null,{\"className\":\"py-16 px-4 bg-gray-50 text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-7xl mx-auto\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold uppercase text-center mb-6\",\"children\":\"Quantitative Comparison\"}],[\"$\",\"div\",null,{\"className\":\"bg-white rounded-lg p-6 border border-gray-200\",\"children\":[\"$\",\"$L1b\",null,{}]}]]}]}]\n"])</script><script>self.__next_f.push([1,"18:[\"$\",\"section\",null,{\"className\":\"py-16 px-4 bg-gray-200 text-gray-900\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold uppercase text-center mb-6\",\"children\":\"Acknowledgment\"}],[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-amber-50 border border-amber-200 rounded-lg p-6 mb-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-3xl\",\"children\":\"üí∞\"}],[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold text-amber-900\",\"children\":\"Funding and GPU Support\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-lg leading-relaxed text-amber-800\",\"children\":\"This research is funded by the Vietnam National Foundation for Science and Technology Development (NAFOSTED) under Grant Number 102.05-2023.31. This research used the GPUs provided by the Intelligent Systems Lab at the Faculty of Information Technology, University of Science, VNU-HCM.\"}]]}],[\"$\",\"div\",null,{\"className\":\"bg-blue-50 border border-blue-200 rounded-lg p-6 mb-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-3xl\",\"children\":\"üôè\"}],[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold text-blue-900\",\"children\":\"User Study Participants\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-lg leading-relaxed text-blue-800\",\"children\":\"We extend our heartfelt gratitude to all participants who took part in our comprehensive user study. Your valuable time, thoughtful feedback, and detailed evaluations were instrumental in validating the effectiveness and usability of our PANDORA framework. Your insights helped us understand the practical impact of our zero-shot object removal approach and provided crucial evidence of its superiority over existing methods.\"}]]}],[\"$\",\"div\",null,{\"className\":\"bg-green-50 border border-green-200 rounded-lg p-6 mb-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-3xl\",\"children\":\"üé®\"}],[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold text-green-900\",\"children\":\"Website Design Inspiration\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-lg leading-relaxed text-green-800\",\"children\":[\"This website design is inspired by\",\" \",[\"$\",\"a\",null,{\"href\":\"https://objectdrop.github.io/\",\"className\":\"text-blue-600 hover:underline font-semibold\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"ObjectDrop\"}],\". We thank the authors for their excellent work and creative design approach.\"]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-purple-50 border border-purple-200 rounded-lg p-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-4\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-3xl\",\"children\":\"üöÄ\"}],[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold text-purple-900\",\"children\":\"Demo Design Inspiration\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-lg leading-relaxed text-purple-800\",\"children\":[\"Our Gradio demo design is inspired by\",\" \",[\"$\",\"a\",null,{\"href\":\"https://huggingface.co/spaces/xichenhku/MimicBrush/tree/main\",\"className\":\"text-blue-600 hover:underline font-semibold\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"MimicBrush\"}],\". We thank the authors for their excellent work and creative design approach.\"]}]]}]]}]]}]}]\n"])</script></body></html>